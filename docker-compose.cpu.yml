# Simple CPU deployment for whisper-large-v3
services:
  whisper-large-v3-cpu:
    build:
      context: .
      dockerfile: Dockerfile.cpu
    image: whisper-large
    container_name: whisper-large
    ports:
      - "8000:8000"
    environment:
      # API Security
      - API_KEYS=${API_KEYS:-your-secret-key}
      
      # CPU Performance (auto-adjust to available CPUs)
      - OMP_NUM_THREADS=${OMP_NUM_THREADS:-8}
      - MKL_NUM_THREADS=${MKL_NUM_THREADS:-8}
      
      # Optional: Override default performance for whisper-1 profile
      # - BEAM_SIZE=5
      # - PATIENCE=1.0
      
    volumes:
      # Model cache (3GB for large-v3)
      - whisper-models:/home/whisper/.cache
      
    deploy:
      resources:
        limits:
          cpus: "8"
          memory: 16G
        reservations:
          cpus: "4"
          memory: 8G
          
    restart: unless-stopped
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s

volumes:
  whisper-models:
    driver: local
